{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# Custom import\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datapath\n",
    "DataPath = \"/PATH/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "def seed_all(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# Multiprocess runs\n",
    "def df_paral_run(func, t_split):\n",
    "    num_cores = np.min([N_cores, len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis = 1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabes\n",
    "Ver = 3 # Version 3\n",
    "Seed = 42\n",
    "seed_all(Seed)\n",
    "lgb_params['seed'] = Seed\n",
    "N_cores = psutil.cpu_count()\n",
    "\n",
    "Target = \"sales\"\n",
    "First_train = 0\n",
    "Last_train = 1941\n",
    "Pred_len = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that will result in overfitting\n",
    "remove_features = [\"id\",\n",
    "                  \"state_id\",\n",
    "                  \"store_id\",\n",
    "                  \"date\",\n",
    "                  \"wm_yr_wk\",\n",
    "                  \"d\",\n",
    "                  \"month\",\n",
    "                  \"year\", \n",
    "                  \"price_nunique\",\n",
    "                  \"price_momentum_y\",\n",
    "                  \"sales_lag_34\",\n",
    "                  \"sales_lag_36\", \n",
    "                   Target]\n",
    "\n",
    "mean_features = [\"enc_cat_id_mean\",\n",
    "                \"enc_cat_id_std\",\n",
    "                \"enc_dept_id_mean\",\n",
    "                \"enc_dept_id_std\",\n",
    "                \"enc_item_id_mean\",\n",
    "                \"enc_item_id_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for features\n",
    "Base     = \"M5_part_1.pkl\"\n",
    "Price    = \"M5_part_2.pkl\"\n",
    "Calendar = \"M5_part_3.pkl\"\n",
    "Lags     = \"lags_df_28.pkl\"\n",
    "Mean_enc = \"mean_encoding_df.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores ids\n",
    "Stores_IDs = [\n",
    "              'CA_1', \n",
    "              'CA_2', \n",
    "              'CA_3', \n",
    "              'CA_4', \n",
    "              'TX_1', \n",
    "              'TX_2', \n",
    "              'TX_3', \n",
    "              'WI_1',\n",
    "              'WI_2', \n",
    "              'WI_3'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits for lags creation\n",
    "Shift_day  = 28\n",
    "N_lags     = 15\n",
    "Lags_split = [col for col in range(Shift_day, Shift_day + N_lags)]\n",
    "Rols_split = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,28,56]:\n",
    "        Rols_split.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load data by store_id\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(Base),\n",
    "                    pd.read_pickle(Price).iloc[:,2:],\n",
    "                    pd.read_pickle(Calendar).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only the corresponding store data\n",
    "    df = df[df[\"store_id\"] == store]\n",
    "    \n",
    "    df2 = pd.read_pickle(Mean_enc)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(Lags).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit\n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit\n",
    "    \n",
    "    # Create feature list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[[\"id\", \"d\", Target] + features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df[\"d\"] >= First_train].reset_index(drop = True)\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in Stores_IDs:\n",
    "        temp_df = pd.read_pickle(\"test_\" + store_id + \".pkl\")\n",
    "        temp_df[\"store_id\"] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dynamic lags and rolling lags\n",
    "def make_lag(lag_day):\n",
    "    lag_df = base_test[[\"id\", \"d\", Target]]\n",
    "    col_name = \"sales_lag_\" + str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby([\"id\"])[Target].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "def make_lag_roll(lag_day):\n",
    "    shift_day = lag_day[0]\n",
    "    roll_win = lag_day[1]\n",
    "    lag_df = base_test[[\"id\", \"d\", Target]]\n",
    "    col_name = \"rolling_mean_tmp_\" + str(shift_day) + \"_\" + str(roll_win)\n",
    "    lag_df[col_name] = lag_df.groupby([\"id\"])[Target].transform(lambda x: x.shift(shift_day).rolling(roll_win).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "[100]\tvalid_0's rmse: 2.01759\n",
      "[200]\tvalid_0's rmse: 1.98668\n",
      "[300]\tvalid_0's rmse: 1.977\n",
      "[400]\tvalid_0's rmse: 1.97009\n",
      "[500]\tvalid_0's rmse: 1.9639\n",
      "[600]\tvalid_0's rmse: 1.95737\n",
      "[700]\tvalid_0's rmse: 1.95216\n",
      "[800]\tvalid_0's rmse: 1.94751\n",
      "[900]\tvalid_0's rmse: 1.9427\n",
      "[1000]\tvalid_0's rmse: 1.93834\n",
      "[1100]\tvalid_0's rmse: 1.9341\n",
      "[1200]\tvalid_0's rmse: 1.92947\n",
      "[1300]\tvalid_0's rmse: 1.92524\n",
      "[1400]\tvalid_0's rmse: 1.92111\n",
      "Train CA_2\n",
      "[100]\tvalid_0's rmse: 1.94771\n",
      "[200]\tvalid_0's rmse: 1.89386\n",
      "[300]\tvalid_0's rmse: 1.87876\n",
      "[400]\tvalid_0's rmse: 1.87048\n",
      "[500]\tvalid_0's rmse: 1.86254\n",
      "[600]\tvalid_0's rmse: 1.85543\n",
      "[700]\tvalid_0's rmse: 1.84901\n",
      "[800]\tvalid_0's rmse: 1.84367\n",
      "[900]\tvalid_0's rmse: 1.8387\n",
      "[1000]\tvalid_0's rmse: 1.83437\n",
      "[1100]\tvalid_0's rmse: 1.8302\n",
      "[1200]\tvalid_0's rmse: 1.82552\n",
      "[1300]\tvalid_0's rmse: 1.82139\n",
      "[1400]\tvalid_0's rmse: 1.81753\n",
      "Train CA_3\n",
      "[100]\tvalid_0's rmse: 2.38517\n",
      "[200]\tvalid_0's rmse: 2.34201\n",
      "[300]\tvalid_0's rmse: 2.32452\n",
      "[400]\tvalid_0's rmse: 2.31584\n",
      "[500]\tvalid_0's rmse: 2.30725\n",
      "[600]\tvalid_0's rmse: 2.30021\n",
      "[700]\tvalid_0's rmse: 2.29376\n",
      "[800]\tvalid_0's rmse: 2.28907\n",
      "[900]\tvalid_0's rmse: 2.28369\n",
      "[1000]\tvalid_0's rmse: 2.27869\n",
      "[1100]\tvalid_0's rmse: 2.27381\n",
      "[1200]\tvalid_0's rmse: 2.26821\n",
      "[1300]\tvalid_0's rmse: 2.26461\n",
      "[1400]\tvalid_0's rmse: 2.26115\n",
      "Train CA_4\n",
      "[100]\tvalid_0's rmse: 1.39211\n",
      "[200]\tvalid_0's rmse: 1.38245\n",
      "[300]\tvalid_0's rmse: 1.37685\n",
      "[400]\tvalid_0's rmse: 1.37269\n",
      "[500]\tvalid_0's rmse: 1.36907\n",
      "[600]\tvalid_0's rmse: 1.36571\n",
      "[700]\tvalid_0's rmse: 1.36269\n",
      "[800]\tvalid_0's rmse: 1.35901\n",
      "[900]\tvalid_0's rmse: 1.35604\n",
      "[1000]\tvalid_0's rmse: 1.35296\n",
      "[1100]\tvalid_0's rmse: 1.3503\n",
      "[1200]\tvalid_0's rmse: 1.34742\n",
      "[1300]\tvalid_0's rmse: 1.34472\n",
      "[1400]\tvalid_0's rmse: 1.3419\n",
      "Train TX_1\n",
      "[100]\tvalid_0's rmse: 1.61328\n",
      "[200]\tvalid_0's rmse: 1.58402\n",
      "[300]\tvalid_0's rmse: 1.57329\n",
      "[400]\tvalid_0's rmse: 1.5662\n",
      "[500]\tvalid_0's rmse: 1.56073\n",
      "[600]\tvalid_0's rmse: 1.55624\n",
      "[700]\tvalid_0's rmse: 1.55251\n",
      "[800]\tvalid_0's rmse: 1.54836\n",
      "[900]\tvalid_0's rmse: 1.54422\n",
      "[1000]\tvalid_0's rmse: 1.54005\n",
      "[1100]\tvalid_0's rmse: 1.536\n",
      "[1200]\tvalid_0's rmse: 1.53205\n",
      "[1300]\tvalid_0's rmse: 1.52828\n",
      "[1400]\tvalid_0's rmse: 1.52487\n",
      "Train TX_2\n",
      "[100]\tvalid_0's rmse: 1.77785\n",
      "[200]\tvalid_0's rmse: 1.75266\n",
      "[300]\tvalid_0's rmse: 1.74156\n",
      "[400]\tvalid_0's rmse: 1.7363\n",
      "[500]\tvalid_0's rmse: 1.73106\n",
      "[600]\tvalid_0's rmse: 1.72691\n",
      "[700]\tvalid_0's rmse: 1.72286\n",
      "[800]\tvalid_0's rmse: 1.71934\n",
      "[900]\tvalid_0's rmse: 1.71633\n",
      "[1000]\tvalid_0's rmse: 1.71138\n",
      "[1100]\tvalid_0's rmse: 1.70762\n",
      "[1200]\tvalid_0's rmse: 1.70456\n",
      "[1300]\tvalid_0's rmse: 1.70092\n",
      "[1400]\tvalid_0's rmse: 1.69752\n",
      "Train TX_3\n",
      "[100]\tvalid_0's rmse: 1.86481\n",
      "[200]\tvalid_0's rmse: 1.81947\n",
      "[300]\tvalid_0's rmse: 1.80523\n",
      "[400]\tvalid_0's rmse: 1.7958\n",
      "[500]\tvalid_0's rmse: 1.78894\n",
      "[600]\tvalid_0's rmse: 1.78208\n",
      "[700]\tvalid_0's rmse: 1.77614\n",
      "[800]\tvalid_0's rmse: 1.77033\n",
      "[900]\tvalid_0's rmse: 1.76646\n",
      "[1000]\tvalid_0's rmse: 1.76056\n",
      "[1100]\tvalid_0's rmse: 1.75628\n",
      "[1200]\tvalid_0's rmse: 1.7513\n",
      "[1300]\tvalid_0's rmse: 1.74764\n",
      "[1400]\tvalid_0's rmse: 1.7437\n",
      "Train WI_1\n",
      "[100]\tvalid_0's rmse: 1.60708\n",
      "[200]\tvalid_0's rmse: 1.58511\n",
      "[300]\tvalid_0's rmse: 1.57596\n",
      "[400]\tvalid_0's rmse: 1.56957\n",
      "[500]\tvalid_0's rmse: 1.56403\n",
      "[600]\tvalid_0's rmse: 1.56018\n",
      "[700]\tvalid_0's rmse: 1.55647\n",
      "[800]\tvalid_0's rmse: 1.55213\n",
      "[900]\tvalid_0's rmse: 1.54842\n",
      "[1000]\tvalid_0's rmse: 1.54498\n",
      "[1100]\tvalid_0's rmse: 1.54152\n",
      "[1200]\tvalid_0's rmse: 1.5385\n",
      "[1300]\tvalid_0's rmse: 1.5353\n",
      "[1400]\tvalid_0's rmse: 1.53219\n",
      "Train WI_2\n",
      "[100]\tvalid_0's rmse: 2.72828\n",
      "[200]\tvalid_0's rmse: 2.63408\n",
      "[300]\tvalid_0's rmse: 2.60916\n",
      "[400]\tvalid_0's rmse: 2.59501\n",
      "[500]\tvalid_0's rmse: 2.58276\n",
      "[600]\tvalid_0's rmse: 2.57318\n",
      "[700]\tvalid_0's rmse: 2.56203\n",
      "[800]\tvalid_0's rmse: 2.55263\n",
      "[900]\tvalid_0's rmse: 2.54599\n",
      "[1000]\tvalid_0's rmse: 2.5377\n",
      "[1100]\tvalid_0's rmse: 2.53046\n",
      "[1200]\tvalid_0's rmse: 2.51992\n",
      "[1300]\tvalid_0's rmse: 2.51097\n",
      "[1400]\tvalid_0's rmse: 2.50321\n",
      "Train WI_3\n",
      "[100]\tvalid_0's rmse: 1.92635\n",
      "[200]\tvalid_0's rmse: 1.87662\n",
      "[300]\tvalid_0's rmse: 1.86037\n",
      "[400]\tvalid_0's rmse: 1.84963\n",
      "[500]\tvalid_0's rmse: 1.84018\n",
      "[600]\tvalid_0's rmse: 1.83362\n",
      "[700]\tvalid_0's rmse: 1.82676\n",
      "[800]\tvalid_0's rmse: 1.82208\n",
      "[900]\tvalid_0's rmse: 1.81635\n",
      "[1000]\tvalid_0's rmse: 1.81125\n",
      "[1100]\tvalid_0's rmse: 1.8069\n",
      "[1200]\tvalid_0's rmse: 1.80124\n",
      "[1300]\tvalid_0's rmse: 1.79756\n",
      "[1400]\tvalid_0's rmse: 1.79283\n"
     ]
    }
   ],
   "source": [
    "## Train models\n",
    "for store_id in Stores_IDs:\n",
    "    print(\"Train\", store_id)\n",
    "    \n",
    "    grid_df, feature_cols = get_data_by_store(store_id)\n",
    "    \n",
    "    train_m = grid_df[\"d\"] <= Last_train\n",
    "    valid_m = train_m & (grid_df[\"d\"] > (Last_train - Pred_len))\n",
    "    pred_m = grid_df[\"d\"] > (Last_train - 100)\n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin to reduce memory spikes during dtype convertations\n",
    "    train_data = lgb.Dataset(grid_df[train_m][feature_cols], label = grid_df[train_m][Target])\n",
    "    train_data.save_binary('train_data.bin')\n",
    "    train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_m][feature_cols], label = grid_df[valid_m][Target])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[pred_m].reset_index(drop = True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle(\"test_\" + store_id + \".pkl\")\n",
    "    del grid_df\n",
    "    \n",
    "    seed_all(Seed)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    model_name = 'lgb_model_' + store_id + '_v' + str(Ver) + '.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "    \n",
    "    # Remove temporary files and objects to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    Model_features = feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  0.64 min round |  0.64 min total |  39873.24 day sales |\n",
      "Predict | Day: 2\n",
      "##########  0.65 min round |  1.29 min total |  37168.73 day sales |\n",
      "Predict | Day: 3\n",
      "##########  0.66 min round |  1.96 min total |  37038.54 day sales |\n",
      "Predict | Day: 4\n",
      "##########  0.72 min round |  2.67 min total |  37049.32 day sales |\n",
      "Predict | Day: 5\n",
      "##########  0.66 min round |  3.34 min total |  42008.78 day sales |\n",
      "Predict | Day: 6\n",
      "##########  0.67 min round |  4.01 min total |  50262.51 day sales |\n",
      "Predict | Day: 7\n",
      "##########  0.66 min round |  4.66 min total |  51103.06 day sales |\n",
      "Predict | Day: 8\n",
      "##########  0.65 min round |  5.31 min total |  44977.70 day sales |\n",
      "Predict | Day: 9\n",
      "##########  0.68 min round |  5.99 min total |  39273.24 day sales |\n",
      "Predict | Day: 10\n",
      "##########  0.66 min round |  6.65 min total |  44033.57 day sales |\n",
      "Predict | Day: 11\n",
      "##########  0.64 min round |  7.29 min total |  45016.40 day sales |\n",
      "Predict | Day: 12\n",
      "##########  0.67 min round |  7.96 min total |  52925.49 day sales |\n",
      "Predict | Day: 13\n",
      "##########  0.67 min round |  8.62 min total |  55773.80 day sales |\n",
      "Predict | Day: 14\n",
      "##########  0.66 min round |  9.29 min total |  58003.98 day sales |\n",
      "Predict | Day: 15\n",
      "##########  0.66 min round |  9.95 min total |  47919.36 day sales |\n",
      "Predict | Day: 16\n",
      "##########  0.67 min round |  10.62 min total |  43648.41 day sales |\n",
      "Predict | Day: 17\n",
      "##########  0.65 min round |  11.27 min total |  42904.87 day sales |\n",
      "Predict | Day: 18\n",
      "##########  0.68 min round |  11.95 min total |  44822.67 day sales |\n",
      "Predict | Day: 19\n",
      "##########  0.68 min round |  12.63 min total |  46776.30 day sales |\n",
      "Predict | Day: 20\n",
      "##########  0.67 min round |  13.30 min total |  57913.43 day sales |\n",
      "Predict | Day: 21\n",
      "##########  0.65 min round |  13.95 min total |  59319.12 day sales |\n",
      "Predict | Day: 22\n",
      "##########  0.70 min round |  14.65 min total |  46393.65 day sales |\n",
      "Predict | Day: 23\n",
      "##########  0.66 min round |  15.31 min total |  43486.21 day sales |\n",
      "Predict | Day: 24\n",
      "##########  0.68 min round |  15.99 min total |  44810.70 day sales |\n",
      "Predict | Day: 25\n",
      "##########  0.66 min round |  16.65 min total |  41424.21 day sales |\n",
      "Predict | Day: 26\n",
      "##########  0.66 min round |  17.32 min total |  45700.44 day sales |\n",
      "Predict | Day: 27\n",
      "##########  0.66 min round |  17.98 min total |  54402.27 day sales |\n",
      "Predict | Day: 28\n",
      "##########  0.65 min round |  18.63 min total |  50018.76 day sales |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>0.894156</td>\n",
       "      <td>0.797862</td>\n",
       "      <td>0.757838</td>\n",
       "      <td>0.815370</td>\n",
       "      <td>1.053641</td>\n",
       "      <td>1.251544</td>\n",
       "      <td>1.214042</td>\n",
       "      <td>1.084730</td>\n",
       "      <td>0.885424</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014416</td>\n",
       "      <td>1.306888</td>\n",
       "      <td>1.140455</td>\n",
       "      <td>0.938197</td>\n",
       "      <td>0.875599</td>\n",
       "      <td>0.869725</td>\n",
       "      <td>0.859978</td>\n",
       "      <td>1.089290</td>\n",
       "      <td>1.307882</td>\n",
       "      <td>1.100659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>0.246908</td>\n",
       "      <td>0.216749</td>\n",
       "      <td>0.207193</td>\n",
       "      <td>0.200287</td>\n",
       "      <td>0.220388</td>\n",
       "      <td>0.289456</td>\n",
       "      <td>0.350437</td>\n",
       "      <td>0.262191</td>\n",
       "      <td>0.244119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.384425</td>\n",
       "      <td>0.427430</td>\n",
       "      <td>0.277320</td>\n",
       "      <td>0.287314</td>\n",
       "      <td>0.293951</td>\n",
       "      <td>0.294857</td>\n",
       "      <td>0.331525</td>\n",
       "      <td>0.410798</td>\n",
       "      <td>0.451908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>0.535192</td>\n",
       "      <td>0.472436</td>\n",
       "      <td>0.492068</td>\n",
       "      <td>0.492642</td>\n",
       "      <td>0.682019</td>\n",
       "      <td>0.913400</td>\n",
       "      <td>0.813372</td>\n",
       "      <td>0.465840</td>\n",
       "      <td>0.506559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756575</td>\n",
       "      <td>0.880776</td>\n",
       "      <td>0.824205</td>\n",
       "      <td>0.505758</td>\n",
       "      <td>0.419118</td>\n",
       "      <td>0.469078</td>\n",
       "      <td>0.511557</td>\n",
       "      <td>0.760734</td>\n",
       "      <td>0.867550</td>\n",
       "      <td>0.840639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.584041</td>\n",
       "      <td>1.354480</td>\n",
       "      <td>1.336962</td>\n",
       "      <td>1.448628</td>\n",
       "      <td>1.829020</td>\n",
       "      <td>2.696900</td>\n",
       "      <td>3.012109</td>\n",
       "      <td>1.856297</td>\n",
       "      <td>1.371992</td>\n",
       "      <td>...</td>\n",
       "      <td>1.738260</td>\n",
       "      <td>2.537276</td>\n",
       "      <td>3.017198</td>\n",
       "      <td>1.766089</td>\n",
       "      <td>1.453292</td>\n",
       "      <td>1.528347</td>\n",
       "      <td>1.550095</td>\n",
       "      <td>1.900432</td>\n",
       "      <td>2.549529</td>\n",
       "      <td>2.699758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>1.117710</td>\n",
       "      <td>0.991719</td>\n",
       "      <td>0.945132</td>\n",
       "      <td>0.996627</td>\n",
       "      <td>1.111623</td>\n",
       "      <td>1.400331</td>\n",
       "      <td>1.463146</td>\n",
       "      <td>1.117716</td>\n",
       "      <td>0.952329</td>\n",
       "      <td>...</td>\n",
       "      <td>1.241647</td>\n",
       "      <td>1.545267</td>\n",
       "      <td>1.517596</td>\n",
       "      <td>1.102651</td>\n",
       "      <td>0.954741</td>\n",
       "      <td>1.008502</td>\n",
       "      <td>1.023550</td>\n",
       "      <td>1.306502</td>\n",
       "      <td>1.504063</td>\n",
       "      <td>1.397712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30485</td>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.551792</td>\n",
       "      <td>0.514148</td>\n",
       "      <td>0.496788</td>\n",
       "      <td>0.541394</td>\n",
       "      <td>0.559960</td>\n",
       "      <td>0.624951</td>\n",
       "      <td>0.726713</td>\n",
       "      <td>0.627854</td>\n",
       "      <td>0.582946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639011</td>\n",
       "      <td>0.759081</td>\n",
       "      <td>0.857963</td>\n",
       "      <td>0.599906</td>\n",
       "      <td>0.641171</td>\n",
       "      <td>0.613696</td>\n",
       "      <td>0.521367</td>\n",
       "      <td>0.524332</td>\n",
       "      <td>0.639211</td>\n",
       "      <td>0.755465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30486</td>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.271574</td>\n",
       "      <td>0.250420</td>\n",
       "      <td>0.246872</td>\n",
       "      <td>0.246384</td>\n",
       "      <td>0.232829</td>\n",
       "      <td>0.269247</td>\n",
       "      <td>0.322886</td>\n",
       "      <td>0.304320</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288918</td>\n",
       "      <td>0.418360</td>\n",
       "      <td>0.459928</td>\n",
       "      <td>0.377799</td>\n",
       "      <td>0.429828</td>\n",
       "      <td>0.433534</td>\n",
       "      <td>0.300204</td>\n",
       "      <td>0.269644</td>\n",
       "      <td>0.316149</td>\n",
       "      <td>0.370912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30487</td>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.663182</td>\n",
       "      <td>0.522572</td>\n",
       "      <td>0.481299</td>\n",
       "      <td>0.435530</td>\n",
       "      <td>0.501278</td>\n",
       "      <td>0.616317</td>\n",
       "      <td>0.690022</td>\n",
       "      <td>0.668807</td>\n",
       "      <td>0.494482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835760</td>\n",
       "      <td>1.245322</td>\n",
       "      <td>1.346179</td>\n",
       "      <td>0.970036</td>\n",
       "      <td>1.016039</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>0.715699</td>\n",
       "      <td>0.733611</td>\n",
       "      <td>0.791815</td>\n",
       "      <td>0.954163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30488</td>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>0.960298</td>\n",
       "      <td>1.063197</td>\n",
       "      <td>1.034226</td>\n",
       "      <td>0.922244</td>\n",
       "      <td>1.094934</td>\n",
       "      <td>1.182236</td>\n",
       "      <td>1.221578</td>\n",
       "      <td>1.284041</td>\n",
       "      <td>1.122904</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074147</td>\n",
       "      <td>1.492655</td>\n",
       "      <td>1.586092</td>\n",
       "      <td>1.230416</td>\n",
       "      <td>1.420511</td>\n",
       "      <td>1.370374</td>\n",
       "      <td>1.154457</td>\n",
       "      <td>1.197202</td>\n",
       "      <td>1.268506</td>\n",
       "      <td>1.401272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30489</td>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1.897154</td>\n",
       "      <td>1.686119</td>\n",
       "      <td>1.590446</td>\n",
       "      <td>1.759036</td>\n",
       "      <td>2.081213</td>\n",
       "      <td>2.065494</td>\n",
       "      <td>1.866456</td>\n",
       "      <td>1.994645</td>\n",
       "      <td>1.788728</td>\n",
       "      <td>...</td>\n",
       "      <td>1.726520</td>\n",
       "      <td>2.025703</td>\n",
       "      <td>2.030723</td>\n",
       "      <td>1.746418</td>\n",
       "      <td>1.880349</td>\n",
       "      <td>1.724252</td>\n",
       "      <td>1.606574</td>\n",
       "      <td>1.909109</td>\n",
       "      <td>1.942903</td>\n",
       "      <td>2.074853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        F1        F2        F3        F4  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  0.894156  0.797862  0.757838  0.815370   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  0.246908  0.216749  0.207193  0.200287   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  0.535192  0.472436  0.492068  0.492642   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  1.584041  1.354480  1.336962  1.448628   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  1.117710  0.991719  0.945132  0.996627   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation  0.551792  0.514148  0.496788  0.541394   \n",
       "30486    FOODS_3_824_WI_3_evaluation  0.271574  0.250420  0.246872  0.246384   \n",
       "30487    FOODS_3_825_WI_3_evaluation  0.663182  0.522572  0.481299  0.435530   \n",
       "30488    FOODS_3_826_WI_3_evaluation  0.960298  1.063197  1.034226  0.922244   \n",
       "30489    FOODS_3_827_WI_3_evaluation  1.897154  1.686119  1.590446  1.759036   \n",
       "\n",
       "             F5        F6        F7        F8        F9  ...       F19  \\\n",
       "0      1.053641  1.251544  1.214042  1.084730  0.885424  ...  1.014416   \n",
       "1      0.220388  0.289456  0.350437  0.262191  0.244119  ...  0.298507   \n",
       "2      0.682019  0.913400  0.813372  0.465840  0.506559  ...  0.756575   \n",
       "3      1.829020  2.696900  3.012109  1.856297  1.371992  ...  1.738260   \n",
       "4      1.111623  1.400331  1.463146  1.117716  0.952329  ...  1.241647   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "30485  0.559960  0.624951  0.726713  0.627854  0.582946  ...  0.639011   \n",
       "30486  0.232829  0.269247  0.322886  0.304320  0.279523  ...  0.288918   \n",
       "30487  0.501278  0.616317  0.690022  0.668807  0.494482  ...  0.835760   \n",
       "30488  1.094934  1.182236  1.221578  1.284041  1.122904  ...  1.074147   \n",
       "30489  2.081213  2.065494  1.866456  1.994645  1.788728  ...  1.726520   \n",
       "\n",
       "            F20       F21       F22       F23       F24       F25       F26  \\\n",
       "0      1.306888  1.140455  0.938197  0.875599  0.869725  0.859978  1.089290   \n",
       "1      0.384425  0.427430  0.277320  0.287314  0.293951  0.294857  0.331525   \n",
       "2      0.880776  0.824205  0.505758  0.419118  0.469078  0.511557  0.760734   \n",
       "3      2.537276  3.017198  1.766089  1.453292  1.528347  1.550095  1.900432   \n",
       "4      1.545267  1.517596  1.102651  0.954741  1.008502  1.023550  1.306502   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.759081  0.857963  0.599906  0.641171  0.613696  0.521367  0.524332   \n",
       "30486  0.418360  0.459928  0.377799  0.429828  0.433534  0.300204  0.269644   \n",
       "30487  1.245322  1.346179  0.970036  1.016039  0.991759  0.715699  0.733611   \n",
       "30488  1.492655  1.586092  1.230416  1.420511  1.370374  1.154457  1.197202   \n",
       "30489  2.025703  2.030723  1.746418  1.880349  1.724252  1.606574  1.909109   \n",
       "\n",
       "            F27       F28  \n",
       "0      1.307882  1.100659  \n",
       "1      0.410798  0.451908  \n",
       "2      0.867550  0.840639  \n",
       "3      2.549529  2.699758  \n",
       "4      1.504063  1.397712  \n",
       "...         ...       ...  \n",
       "30485  0.639211  0.755465  \n",
       "30486  0.316149  0.370912  \n",
       "30487  0.791815  0.954163  \n",
       "30488  1.268506  1.401272  \n",
       "30489  1.942903  2.074853  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with a small part of the training data to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time\n",
    "main_time = time.time()\n",
    "\n",
    "for pred_day in range(1, 29):\n",
    "    print(\"Predict | Day:\", pred_day)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    grid_df = pd.concat([grid_df, df_paral_run(make_lag_roll, Rols_split)], axis=1)\n",
    "    \n",
    "    for store_id in Stores_IDs:\n",
    "        model_path = \"lgb_model_\" + store_id + \"_v\" + str(Ver) + \".bin\" \n",
    "        estimator = pickle.load(open(model_path, \"rb\"))\n",
    "        \n",
    "        day_m = base_test['d'] == (Last_train + pred_day)\n",
    "        store_m = base_test['store_id']==store_id\n",
    "        \n",
    "        m = (day_m) & (store_m)\n",
    "        base_test[Target][m] = estimator.predict(grid_df[m][Model_features])\n",
    "        \n",
    "    # Make column name and add to all_preds DF\n",
    "    temp_df = base_test[day_m][[\"id\", Target]]\n",
    "    temp_df.columns = [\"id\", \"F\" + str(pred_day)]\n",
    "    if \"id\" in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on = [\"id\"], how = \"left\")\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "              ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "              ' %0.2f day sales |' % (temp_df['F' + str(pred_day)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data\n",
    "submission = pd.read_csv(DataPath + \"/sample_submission.csv\")[[\"id\"]]\n",
    "submission = submission.merge(all_preds, on=['id'], how = \"left\").fillna(0)\n",
    "submission.to_csv('submission_v' + str(Ver) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n",
    "                 calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 'all'  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n",
    "                     .columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n",
    "                               .columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], \n",
    "                                 axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n",
    "                    [valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n",
    "                    .set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index()\\\n",
    "                   .rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left',\n",
    "                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n",
    "                    .unstack(level=2)['value']\\\n",
    "                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n",
    "                    .reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns],\n",
    "                               weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt) \n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, \n",
    "                                       np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape \\\n",
    "               == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, \n",
    "                                       columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], \n",
    "                                 valid_preds], axis=1, sort=False)\n",
    "\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "\n",
    "            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n",
    "            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n",
    "            \n",
    "            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n",
    "            setattr(self, f'lv{i + 1}_scores', lv_scores)\n",
    "            \n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, \n",
    "                                  sort=False).prod(axis=1)\n",
    "            \n",
    "            all_scores.append(lv_scores.sum())\n",
    "            \n",
    "        self.all_scores = all_scores\n",
    "\n",
    "        return np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
