{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions to reduce memory and check momory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since the data is pretty large, we can convert data type to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage\n",
    "def memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2)\n",
    "\n",
    "def size_fmt(num, suffix = \"B\"):\n",
    "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, \"Yi\", suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Reducer\n",
    "\n",
    "def reduce_mem_usage(df, verbose = True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32) \n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\n",
    "              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on = merge_on, how = 'left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables and data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "Target = \"sales\"\n",
    "Last_train = 1941\n",
    "iden_index = [\"id\",\"d\"] # columns that can be used as an identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datapath\n",
    "DataPath = \"/PATH/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Data\")\n",
    "train_df = pd.read_csv(DataPath + \"/sales_train_evaluation.csv\")\n",
    "prices_df = pd.read_csv(DataPath + \"/sell_prices.csv\")\n",
    "calendar_df = pd.read_csv(DataPath + \"/calendar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "horizontal train rows 30490\n",
      "vertical train rows 30490\n"
     ]
    }
   ],
   "source": [
    "# Transform horizontal into vertical data to fit the model\n",
    "# indexes: \"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"\n",
    "# labels: \"d_\"\n",
    "print(\"Create Grid\")\n",
    "idx_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "grid_df = pd.melt(train_df, id_vars = idx_cols, var_name = 'd', value_name = Target)\n",
    "\n",
    "print(f\"horizontal train rows {len(train_df)}\")\n",
    "print(f\"vertical train rows {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test data into grid in order to make predictions\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1, 29):\n",
    "    temp_df = train_df[idx_cols]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = \"d_\" + str(Last_train + i)\n",
    "    temp_df[Target] = np.nan\n",
    "    add_grid = pd.concat([add_grid, temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temporary dfs and original data\n",
    "del temp_df, add_grid\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Original grid_df:   3.6GiB\n",
      "     Reduced grid_df:   1.3GiB\n"
     ]
    }
   ],
   "source": [
    "# Check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',size_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "# Free memory by converting \"string\" to categorical data\n",
    "for col in idx_cols:\n",
    "    grid_df[col] = grid_df[col].astype(\"category\")\n",
    "# Check memory usage now\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',size_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First week that a product was in store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A lot of 0 sales were observed in the dataset, and many of them are because \n",
    " the products were not in store at that time. From the price table we are able\n",
    " to get this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n"
     ]
    }
   ],
   "source": [
    "# Discover the first week that a product was in store\n",
    "print(\"Release week\")\n",
    "release_df = prices_df.groupby([\"store_id\", \"item_id\"])[\"wm_yr_wk\"].agg([\"min\"]).reset_index()\n",
    "release_df.columns = [\"store_id\", \"item_id\", \"release\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge release info into grid_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, [\"store_id\", \"item_id\"])\n",
    "del release_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Original grid_df:   1.8GiB\n",
      "     Reduced grid_df:   1.5GiB\n"
     ]
    }
   ],
   "source": [
    "# Remove the rows if the week is earlier than release week from grid\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[[\"wm_yr_wk\", \"d\"]], [\"d\"])\n",
    "grid_df = grid_df[grid_df[\"wm_yr_wk\"] >= grid_df[\"release\"]]\n",
    "grid_df = grid_df.reset_index(drop = True)\n",
    "\n",
    "# Check memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',size_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "grid_df[\"release\"] = grid_df[\"release\"] - grid_df[\"release\"].min()\n",
    "grid_df[\"release\"] = grid_df[\"release\"].astype(np.int16)\n",
    "\n",
    "# Check memory usage after converting\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',size_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part1\n",
      "Size: (47735397, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Save part1\")\n",
    "grid_df.to_pickle(\"M5_part_1.pkl\")\n",
    "print(\"Size:\", grid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prices and feature engineering for prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices features\n"
     ]
    }
   ],
   "source": [
    "# Prices\n",
    "print(\"Prices features\")\n",
    "\n",
    "# Basic aggregation: price min, max, mean, std\n",
    "prices_df[\"price_min\"] = prices_df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].transform(\"min\")\n",
    "prices_df[\"price_max\"] = prices_df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].transform(\"max\")\n",
    "prices_df[\"price_mean\"] = prices_df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].transform(\"mean\")\n",
    "prices_df[\"price_std\"] = prices_df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].transform(\"std\")\n",
    "\n",
    "# Price normalization\n",
    "prices_df[\"sell_price\"] = prices_df[\"sell_price\"] / prices_df[\"price_max\"]\n",
    "\n",
    "# Some prices are inflated and some are stable\n",
    "prices_df[\"price_nunique\"] = prices_df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].transform(\"nunique\")\n",
    "prices_df[\"item_nunique\"] = prices_df.groupby([\"store_id\", \"sell_price\"])[\"item_id\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n",
       "0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n",
       "1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n",
       "2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n",
       "3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n",
       "4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0          NaN          NaN          NaN        0        0        0  \n",
       "1          NaN          NaN          NaN        0        0        0  \n",
       "2          NaN          NaN          NaN        0        0        0  \n",
       "3          NaN          NaN          NaN        1        1        0  \n",
       "4          NaN          NaN          NaN        1        0        1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having month and year as windows and do some rolling aggregation\n",
    "calendar_prices = calendar_df[[\"wm_yr_wk\", \"month\", \"year\"]]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset = [\"wm_yr_wk\"])\n",
    "prices_df = prices_df.merge(calendar_prices, on = [\"wm_yr_wk\"], how = \"left\")\n",
    "\n",
    "del calendar_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price shifted by week, month mean and year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1867.97 Mb (64.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Merge prices and save part 2\n",
    "original_cols = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], how = \"left\")\n",
    "keep_cols = [col for col in list(grid_df) if col not in original_cols]\n",
    "grid_df = grid_df[iden_index + keep_cols]\n",
    "grid_df = reduce_mem_usage(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (47735397, 14)\n"
     ]
    }
   ],
   "source": [
    "# Save part2\n",
    "grid_df.to_pickle('M5_part_2.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "del prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = pd.read_pickle(\"M5_part_1.pkl\")\n",
    "grid_df = grid_df[iden_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load calendar and feature engineering for calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge calendar\n",
    "icols = [\"date\",\n",
    "         \"d\",\n",
    "         \"event_name_1\",\n",
    "         \"event_type_1\",\n",
    "         \"event_name_2\",\n",
    "         \"event_type_2\",\n",
    "         \"snap_CA\",\n",
    "         \"snap_TX\",\n",
    "         \"snap_WI\"]\n",
    "grid_df = grid_df.merge(calendar_df[icols], on = \"d\", how = \"left\")\n",
    "\n",
    "# Convert data types to save memory\n",
    "icols = [\"event_name_1\",\n",
    "         \"event_type_1\",\n",
    "         \"event_name_2\",\n",
    "         \"event_type_2\",\n",
    "         \"snap_CA\",\n",
    "         \"snap_TX\",\n",
    "         \"snap_WI\"]\n",
    "\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date and date features\n",
    "grid_df[\"date\"] = pd.to_datetime(grid_df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>date</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HOBBIES_1_010_CA_1_evaluation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>HOBBIES_1_012_CA_1_evaluation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>HOBBIES_1_015_CA_1_evaluation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id    d       date event_name_1 event_type_1  \\\n",
       "0  HOBBIES_1_008_CA_1_evaluation  d_1 2011-01-29          NaN          NaN   \n",
       "1  HOBBIES_1_009_CA_1_evaluation  d_1 2011-01-29          NaN          NaN   \n",
       "2  HOBBIES_1_010_CA_1_evaluation  d_1 2011-01-29          NaN          NaN   \n",
       "3  HOBBIES_1_012_CA_1_evaluation  d_1 2011-01-29          NaN          NaN   \n",
       "4  HOBBIES_1_015_CA_1_evaluation  d_1 2011-01-29          NaN          NaN   \n",
       "\n",
       "  event_name_2 event_type_2 snap_CA snap_TX snap_WI  \n",
       "0          NaN          NaN       0       0       0  \n",
       "1          NaN          NaN       0       0       0  \n",
       "2          NaN          NaN       0       0       0  \n",
       "3          NaN          NaN       0       0       0  \n",
       "4          NaN          NaN       0       0       0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive day, week, month, year info from calendar data\n",
    "grid_df[\"tm_d\"] = grid_df[\"date\"].dt.day.astype(np.int8)\n",
    "grid_df[\"tm_w\"] = grid_df[\"date\"].dt.week.astype(np.int8)\n",
    "grid_df[\"tm_m\"] = grid_df[\"date\"].dt.month.astype(np.int8)\n",
    "grid_df[\"tm_y\"] = grid_df[\"date\"].dt.year\n",
    "grid_df[\"tm_y\"] = (grid_df[\"tm_y\"] - grid_df[\"tm_y\"].min()).astype(np.int8)\n",
    "grid_df[\"tm_wm\"] = grid_df[\"tm_d\"].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df[\"tm_dw\"] = grid_df[\"date\"].dt.dayofweek.astype(np.int8)\n",
    "grid_df[\"tm_w_end\"] = (grid_df[\"tm_dw\"] >= 5).astype(np.int8)\n",
    "\n",
    "del grid_df[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part 3\n",
      "Size: (47735397, 16)\n"
     ]
    }
   ],
   "source": [
    "# Save part 3\n",
    "print(\"Save part 3\")\n",
    "grid_df.to_pickle('M5_part_3.pkl')\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional cleaning\n",
    "grid_df = pd.read_pickle('M5_part_1.pkl')\n",
    "grid_df[\"d\"] = grid_df[\"d\"].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove \"wm_yr_wk\"\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('M5_part_1.pkl')\n",
    "\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features\n",
    "grid_df = pd.read_pickle('M5_part_1.pkl')\n",
    "\n",
    "# Need \"id\", \"d\", sales to make lags and rollings\n",
    "grid_df = grid_df[[\"id\", \"d\", \"sales\"]]\n",
    "shift_day = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create lag features\n",
      "6.41 min: Lags\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Create lag features\")\n",
    "\n",
    "lag_days = [col for col in range(shift_day, shift_day + 15)]\n",
    "grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lag_days\n",
    "        for col in [Target]\n",
    "    })\n",
    "\n",
    "# Save some memory\n",
    "for col in list(grid_df):\n",
    "    if 'lag' in col:\n",
    "        grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create rolling features\n",
      "Rolling period: 7\n",
      "Rolling period: 14\n",
      "Rolling period: 28\n",
      "Rolling period: 56\n",
      "Rolling period: 182\n",
      "Rolling period: 364\n",
      "6.67 min: Lags\n"
     ]
    }
   ],
   "source": [
    "# Rolling features\n",
    "start_time = time.time()\n",
    "print(\"Create rolling features\")\n",
    "\n",
    "for i in [7, 14, 28, 56, 182, 364]:\n",
    "    print(\"Rolling period:\" ,i)\n",
    "    grid_df[\"rolling_mean_\" + str(i)] = grid_df.groupby([\"id\"])[Target].transform(lambda x: x.shift(shift_day).rolling(i).mean()).astype(np.float16)\n",
    "    grid_df[\"rolling_std_\" + str(i)] = grid_df.groupby([\"id\"])[Target].transform(lambda x: x.shift(shift_day).rolling(i).std()).astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift period: 1\n",
      "Shift period: 7\n",
      "Shift period: 14\n",
      "6.75 min: Lags\n"
     ]
    }
   ],
   "source": [
    "# Rolling with sliding shift\n",
    "start_time = time.time()\n",
    "for d_shift in [1, 7, 14]:\n",
    "    print(\"Shift period:\", d_shift)\n",
    "    for d_window in [7, 14, 28, 56]:\n",
    "        col_name = \"rolling_mean_tmp_\" + str(d_shift) + \"_\" + str(d_window)\n",
    "        grid_df[col_name] = grid_df.groupby([\"id\"])[Target].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save lags and rollings\n"
     ]
    }
   ],
   "source": [
    "# Emport the data\n",
    "print(\"Save lags and rollings\")\n",
    "grid_df.to_pickle(\"lags_df_\" + str(shift_day) + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>sales_lag_28</th>\n",
       "      <th>sales_lag_29</th>\n",
       "      <th>sales_lag_30</th>\n",
       "      <th>sales_lag_31</th>\n",
       "      <th>sales_lag_32</th>\n",
       "      <th>sales_lag_33</th>\n",
       "      <th>sales_lag_34</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_mean_tmp_1_28</th>\n",
       "      <th>rolling_mean_tmp_1_56</th>\n",
       "      <th>rolling_mean_tmp_7_7</th>\n",
       "      <th>rolling_mean_tmp_7_14</th>\n",
       "      <th>rolling_mean_tmp_7_28</th>\n",
       "      <th>rolling_mean_tmp_7_56</th>\n",
       "      <th>rolling_mean_tmp_14_7</th>\n",
       "      <th>rolling_mean_tmp_14_14</th>\n",
       "      <th>rolling_mean_tmp_14_28</th>\n",
       "      <th>rolling_mean_tmp_14_56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HOBBIES_1_010_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>HOBBIES_1_012_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>HOBBIES_1_015_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  d  sales  sales_lag_28  sales_lag_29  \\\n",
       "0  HOBBIES_1_008_CA_1_evaluation  1   12.0           NaN           NaN   \n",
       "1  HOBBIES_1_009_CA_1_evaluation  1    2.0           NaN           NaN   \n",
       "2  HOBBIES_1_010_CA_1_evaluation  1    0.0           NaN           NaN   \n",
       "3  HOBBIES_1_012_CA_1_evaluation  1    0.0           NaN           NaN   \n",
       "4  HOBBIES_1_015_CA_1_evaluation  1    4.0           NaN           NaN   \n",
       "\n",
       "   sales_lag_30  sales_lag_31  sales_lag_32  sales_lag_33  sales_lag_34  ...  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "1           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "2           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "3           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "4           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "\n",
       "   rolling_mean_tmp_1_28  rolling_mean_tmp_1_56  rolling_mean_tmp_7_7  \\\n",
       "0                    NaN                    NaN                   NaN   \n",
       "1                    NaN                    NaN                   NaN   \n",
       "2                    NaN                    NaN                   NaN   \n",
       "3                    NaN                    NaN                   NaN   \n",
       "4                    NaN                    NaN                   NaN   \n",
       "\n",
       "   rolling_mean_tmp_7_14  rolling_mean_tmp_7_28  rolling_mean_tmp_7_56  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   rolling_mean_tmp_14_7  rolling_mean_tmp_14_14  rolling_mean_tmp_14_28  \\\n",
       "0                    NaN                     NaN                     NaN   \n",
       "1                    NaN                     NaN                     NaN   \n",
       "2                    NaN                     NaN                     NaN   \n",
       "3                    NaN                     NaN                     NaN   \n",
       "4                    NaN                     NaN                     NaN   \n",
       "\n",
       "   rolling_mean_tmp_14_56  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all the lag features, rolling means and rolling means for sliding windows\n",
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "# Mean encoding\n",
    "grid_df = pd.read_pickle(\"M5_part_1.pkl\")\n",
    "grid_df[Target][grid_df[\"d\"] > (Last_train - 28)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "mcols = [['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']]\n",
    "\n",
    "for col in mcols:\n",
    "    print(\"Encoding\", col)\n",
    "    col_name = \"_\" + \"_\".join(col) + \"_\"\n",
    "    grid_df[\"enc\" + col_name + \"mean\"] = grid_df.groupby(col)[Target].transform(\"mean\").astype(np.float16)\n",
    "    grid_df[\"enc\" + col_name + \"std\"] = grid_df.groupby(col)[Target].transform(\"std\").astype(np.float16)\n",
    "    \n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[[\"id\", \"d\"] + keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46881677 entries, 0 to 46881676\n",
      "Data columns (total 24 columns):\n",
      "id                           category\n",
      "d                            int16\n",
      "enc_state_id_mean            float16\n",
      "enc_state_id_std             float16\n",
      "enc_store_id_mean            float16\n",
      "enc_store_id_std             float16\n",
      "enc_cat_id_mean              float16\n",
      "enc_cat_id_std               float16\n",
      "enc_dept_id_mean             float16\n",
      "enc_dept_id_std              float16\n",
      "enc_state_id_cat_id_mean     float16\n",
      "enc_state_id_cat_id_std      float16\n",
      "enc_state_id_dept_id_mean    float16\n",
      "enc_state_id_dept_id_std     float16\n",
      "enc_store_id_cat_id_mean     float16\n",
      "enc_store_id_cat_id_std      float16\n",
      "enc_store_id_dept_id_mean    float16\n",
      "enc_store_id_dept_id_std     float16\n",
      "enc_item_id_mean             float16\n",
      "enc_item_id_std              float16\n",
      "enc_item_id_state_id_mean    float16\n",
      "enc_item_id_state_id_std     float16\n",
      "enc_item_id_store_id_mean    float16\n",
      "enc_item_id_store_id_std     float16\n",
      "dtypes: category(1), float16(22), int16(1)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "# Export the data\n",
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features = [\"enc_cat_id_mean\",\n",
    "                \"enc_cat_id_std\",\n",
    "                \"enc_dept_id_mean\",\n",
    "                \"enc_dept_id_std\",\n",
    "                \"enc_item_id_mean\",\n",
    "                \"enc_item_id_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom features and feature selection\n",
    "# Read data\n",
    "grid_df = pd.concat([pd.read_pickle(\"M5_part_1.pkl\"),\n",
    "                     pd.read_pickle(\"M5_part_2.pkl\").iloc[:,2:],\n",
    "                     pd.read_pickle(\"M5_part_3.pkl\").iloc[:,2:],\n",
    "                     pd.read_pickle(\"mean_encoding_df.pkl\")[mean_features]],\n",
    "                     axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling to make calculations faster\n",
    "# Keep only 5% of the id\n",
    "keep_id = np.array_split(list(grid_df[\"id\"].unique()),20)[0]\n",
    "grid_df = grid_df[grid_df[\"id\"].isin(keep_id)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "Seed = 42 # Random seed for everything\n",
    "random.seed(Seed)\n",
    "np.random.seed(Seed)\n",
    "N_cores = psutil.cpu_count() # Avaliable CPU cores\n",
    "\n",
    "# Drop the data from test part\n",
    "grid_df = grid_df[grid_df[\"d\"] <= Last_train].reset_index(drop = True)\n",
    "\n",
    "# Features exclude from the training\n",
    "remove_features = [\"id\",\"d\",Target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model would help check new features performance\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
    "                    'objective': 'regression',       # Standart loss for RMSE\n",
    "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
    "                    'subsample': 0.8,                \n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
    "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
    "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
    "                    'feature_fraction': 0.8,\n",
    "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
    "                    'seed': Seed,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "# RMSE\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "def make_fast_test(df):\n",
    "    features_cols = [col for col in list(grid_df) if col not in remove_features]\n",
    "    # Use last 28 days data as validation data\n",
    "    train_X = df[df.d <= (Last_train - 28)][features_cols]\n",
    "    train_y = df[df.d <= (Last_train - 28)][Target]\n",
    "    valid_X = df[df.d > (Last_train - 28)][features_cols]\n",
    "    valid_y = df[df.d > (Last_train - 28)][Target]\n",
    "    \n",
    "    train_data = lgb.Dataset(train_X, label = train_y)\n",
    "    valid_data = lgb.Dataset(valid_X, label = valid_y)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                        lgb_params,\n",
    "                        train_data,\n",
    "                        valid_sets = [train_data,valid_data],\n",
    "                        verbose_eval = 500,\n",
    "                    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's rmse: 2.74769\tvalid_1's rmse: 2.36255\n",
      "Early stopping, best iteration is:\n",
      "[545]\ttraining's rmse: 2.7338\tvalid_1's rmse: 2.36154\n"
     ]
    }
   ],
   "source": [
    "# Make baseline model\n",
    "baseline_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard RMSE 2.361544573219499\n"
     ]
    }
   ],
   "source": [
    "# Permutation importance test\n",
    "# Create data features and validation data \n",
    "features_cols = [col for col in list(grid_df) if col not in remove_features]\n",
    "valid_df = grid_df[grid_df['d'] > (Last_train - 28)].reset_index(drop=True)\n",
    "\n",
    "# Make normal prediction with our model and save score\n",
    "valid_df['preds'] = baseline_model.predict(valid_df[features_cols])\n",
    "base_score = rmse(valid_df[Target], valid_df['preds'])\n",
    "print('Standard RMSE', base_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release 0.0 0.0 0.0\n",
      "sell_price 0.0219 0.0312 0.0148\n",
      "price_min 0.0151 0.0193 0.01\n",
      "price_max 0.0027 0.0044 0.001\n",
      "price_mean -0.0001 0.0015 -0.0016\n",
      "price_std 0.0239 0.0275 0.0198\n",
      "price_nunique 0.0009 0.0047 -0.0017\n",
      "item_nunique 0.0193 0.0319 0.0065\n",
      "month 0.0015 0.003 0.0002\n",
      "year 0.0 0.0 0.0\n",
      "prices_momentum 0.0004 0.0024 -0.0014\n",
      "price_momentum 0.0 0.0004 -0.0005\n",
      "price_momentum_m 0.0308 0.0438 0.0187\n",
      "price_momentum_y 0.007 0.0215 0.0005\n",
      "tm_d 0.0052 0.01 0.0008\n",
      "tm_w 0.0036 0.0056 0.0007\n",
      "tm_m 0.0014 0.0032 -0.0012\n",
      "tm_y 0.0 0.0 0.0\n",
      "tm_wm -0.0002 0.0009 -0.0008\n",
      "tm_dw 0.1581 0.1879 0.1302\n",
      "tm_w_end 0.0149 0.022 0.0043\n",
      "enc_cat_id_mean 0.0 0.0002 -0.0001\n",
      "enc_cat_id_std 0.0 0.0 0.0\n",
      "enc_dept_id_mean 0.0032 0.0042 0.0022\n",
      "enc_dept_id_std 0.0007 0.0013 0.0001\n",
      "enc_item_id_mean 0.4706 0.4854 0.4598\n",
      "enc_item_id_std 0.0468 0.0519 0.0424\n"
     ]
    }
   ],
   "source": [
    "# Looping over all numerical features\n",
    "for col in features_cols:\n",
    "    # We will make validation set copy to restore features states on each run\n",
    "    temp_df = valid_df.copy()\n",
    "    \n",
    "    if temp_df[col].dtype.name != \"category\":\n",
    "        score_list = []\n",
    "        for i in range(0, 50):\n",
    "            temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "            temp_df[\"preds\"] = baseline_model.predict(temp_df[features_cols])\n",
    "            cur_score = rmse(temp_df[Target], temp_df[\"preds\"])\n",
    "            score_list.append(cur_score)\n",
    "            #print(score_list)\n",
    "        \n",
    "        print(col, np.round(0.02 * sum(score_list) - base_score, 4), \n",
    "              np.round(max(score_list) - base_score, 4),\n",
    "              np.round(min(score_list) - base_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temp data\n",
    "del temp_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
